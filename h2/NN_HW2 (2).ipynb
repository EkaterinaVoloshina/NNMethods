{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "NN_HW2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hu9ijKnqpTEt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94083087-dbc0-4dd0-8e34-a61355d3a965"
      },
      "source": [
        "!wget https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv\n",
        "!wget https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-26 19:30:36--  https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/r6u59ljhhjdg6j0/negative.csv [following]\n",
            "--2021-11-26 19:30:36--  https://www.dropbox.com/s/raw/r6u59ljhhjdg6j0/negative.csv\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2021-11-26 19:30:36 ERROR 404: Not Found.\n",
            "\n",
            "--2021-11-26 19:30:36--  https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:601b:18::a27d:812\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/fnpq3z4bcnoktiv/positive.csv [following]\n",
            "--2021-11-26 19:30:36--  https://www.dropbox.com/s/raw/fnpq3z4bcnoktiv/positive.csv\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2021-11-26 19:30:37 ERROR 404: Not Found.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQBG6ih2RGsQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d25a17f9-3501-45d0-8eef-9b1c256f68d6"
      },
      "source": [
        "! pip install pymorphy2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.7/dist-packages (0.9.1)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.6.2)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.7.2)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (2.4.417127.4579844)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hk1BeVu_qso_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1eefb71c-84ed-4149-8594-d6aa8a763915"
      },
      "source": [
        "! pip install torchmetrics"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.7/dist-packages (0.6.0)\n",
            "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.10.0+cu111)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.1->torchmetrics) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5Mlu_QHpbor"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "import random\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.optim as optim\n",
        "from torchmetrics.functional import f1\n",
        "\n",
        "tokenizer = WordPunctTokenizer()\n",
        "morph = MorphAnalyzer()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aw3XXst0pyFJ"
      },
      "source": [
        "def seed_everything(seed: int): # код из тетрадки по иаду, чтобы зафиксировать все\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X95RCNhxpzj2"
      },
      "source": [
        "seed_everything(42)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhOGyoOH4W5J"
      },
      "source": [
        "# Часть 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxWRPqUvSgMF"
      },
      "source": [
        "Создаем датасет:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNiEQaj-qhWC"
      },
      "source": [
        "pos_tweets = pd.read_csv('positive.csv', encoding='utf-8', sep=';', header=None,  names=[0,1,2,'text','tone',5,6,7,8,9,10,11])\n",
        "neg_tweets = pd.read_csv('negative.csv', encoding='utf-8', sep=';', header=None, names=[0,1,2,'text','tone',5,6,7,8,9,10,11] )\n",
        "neg_tweets['tone'] = 0"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QT3_bZgLrJQA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68045d5b-694c-4829-8755-e9d06d3986b6"
      },
      "source": [
        "all_tweets_data = pos_tweets.append(neg_tweets)\n",
        "print(len(all_tweets_data))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "226834\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q06Mkc7uqksr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "352f9e74-2190-44e1-abd2-77251d1e1dfb"
      },
      "source": [
        "all_tweets_data.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>text</th>\n",
              "      <th>tone</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>408906692374446080</td>\n",
              "      <td>1386325927</td>\n",
              "      <td>pleease_shut_up</td>\n",
              "      <td>@first_timee хоть я и школота, но поверь, у на...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7569</td>\n",
              "      <td>62</td>\n",
              "      <td>61</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>408906692693221377</td>\n",
              "      <td>1386325927</td>\n",
              "      <td>alinakirpicheva</td>\n",
              "      <td>Да, все-таки он немного похож на него. Но мой ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>11825</td>\n",
              "      <td>59</td>\n",
              "      <td>31</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>408906695083954177</td>\n",
              "      <td>1386325927</td>\n",
              "      <td>EvgeshaRe</td>\n",
              "      <td>RT @KatiaCheh: Ну ты идиотка) я испугалась за ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1273</td>\n",
              "      <td>26</td>\n",
              "      <td>27</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>408906695356973056</td>\n",
              "      <td>1386325927</td>\n",
              "      <td>ikonnikova_21</td>\n",
              "      <td>RT @digger2912: \"Кто то в углу сидит и погибае...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1549</td>\n",
              "      <td>19</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>408906761416867842</td>\n",
              "      <td>1386325943</td>\n",
              "      <td>JumpyAlex</td>\n",
              "      <td>@irina_dyshkant Вот что значит страшилка :D\\nН...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>597</td>\n",
              "      <td>16</td>\n",
              "      <td>23</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   0           1                2   ...  9   10  11\n",
              "0  408906692374446080  1386325927  pleease_shut_up  ...  62  61   0\n",
              "1  408906692693221377  1386325927  alinakirpicheva  ...  59  31   2\n",
              "2  408906695083954177  1386325927        EvgeshaRe  ...  26  27   0\n",
              "3  408906695356973056  1386325927    ikonnikova_21  ...  19  17   0\n",
              "4  408906761416867842  1386325943        JumpyAlex  ...  16  23   1\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCuwRnBSFqsQ"
      },
      "source": [
        "def tokenize(text):\n",
        "    #tokens = tokenizer.tokenize(text.lower())\n",
        "    tokens = tokenizer.tokenize(text.lower())\n",
        "    lemmas = [morph.parse(token)[0].normal_form for token in tokens if token not in punctuation]\n",
        "    return ' '.join(lemmas)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19JS73hkVivP"
      },
      "source": [
        "all_tweets_data['lemmas'] = all_tweets_data['text'].apply(tokenize)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLS2A-Tprqgy"
      },
      "source": [
        "tweets_data = shuffle(all_tweets_data[['text','tone', 'lemmas']])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYP7ja6HrsWh"
      },
      "source": [
        "train_sentences, val_sentences = train_test_split(tweets_data, test_size=0.2)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QLlEqsxSlli"
      },
      "source": [
        "Теперь создадим vocab2id для токенов, лемм и символов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1-QZKkRrxm7"
      },
      "source": [
        "def preprocess(text):\n",
        "    tokens = text.lower().split()\n",
        "    tokens = [token.strip(punctuation) for token in tokens]\n",
        "    return tokens"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6Pw4zeyr0vf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e433cee7-85fb-4763-9125-ff2dcb10849d"
      },
      "source": [
        "word_vocab = Counter()\n",
        "lemma_vocab = Counter()\n",
        "symbol_vocab = Counter()\n",
        "\n",
        "for i, text in tweets_data.iterrows():\n",
        "    word_vocab.update(preprocess(text[0]))\n",
        "    lemma_vocab.update(text[2].split())\n",
        "    symbol_vocab.update(list(text[0]))\n",
        "print('всего уникальных токенов:', len(word_vocab))\n",
        "print('всего уникальных лемм:', len(lemma_vocab))\n",
        "print('всего уникальных символов:', len(symbol_vocab))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "всего уникальных токенов: 366250\n",
            "всего уникальных лемм: 224582\n",
            "всего уникальных символов: 456\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_K3CYQWpGMdt"
      },
      "source": [
        "def create2id(vocab, threshold):\n",
        "    filtered_vocab = set()\n",
        "    for word in vocab:\n",
        "        if vocab[word] > threshold:\n",
        "            filtered_vocab.add(word)\n",
        "    print(f'уникальных токенов, втретившихся больше {threshold} раз:', len(filtered_vocab))\n",
        "\n",
        "    word2id = {'PAD':0}\n",
        "    for word in filtered_vocab:\n",
        "        word2id[word] = len(word2id)\n",
        "    id2word = {i:word for word, i in word2id.items()}\n",
        "    return word2id, id2word"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhQCrYcBGepd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d1a1c71-e0ea-4436-dfbf-97764ead0114"
      },
      "source": [
        "word2id, id2word = create2id(word_vocab, 2)\n",
        "lemma2id, id2lemma = create2id(lemma_vocab, 2)\n",
        "symbol2id, id2symbol = create2id(symbol_vocab, 5)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "уникальных токенов, втретившихся больше 2 раз: 64423\n",
            "уникальных токенов, втретившихся больше 2 раз: 45069\n",
            "уникальных токенов, втретившихся больше 5 раз: 234\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af7LzOeStYdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8a75897-27bb-4f14-fe94-8bdbe3910031"
      },
      "source": [
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "DEVICE"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4dVa2ZVSsFk"
      },
      "source": [
        "# Часть I.\n",
        "\n",
        "\n",
        "Зададим датасет:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hm_0JhQruvKx"
      },
      "source": [
        "class TweetsDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataset, word2id, DEVICE, text='text'):\n",
        "        self.dataset = dataset[text].values\n",
        "        self.word2id = word2id\n",
        "        self.length = dataset.shape[0]\n",
        "        self.target = torch.Tensor(dataset['tone'].values)\n",
        "        self.device = DEVICE\n",
        "\n",
        "\n",
        "    def __len__(self): #это обязательный метод, он должен уметь считать длину датасета\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, index): #еще один обязательный метод. По индексу возвращает элемент выборки\n",
        "        tokens = self.preprocess(self.dataset[index]) # токенизируем\n",
        "        ids = torch.LongTensor([self.word2id[token] for token in tokens if token in self.word2id])\n",
        "        y = self.target[index]\n",
        "        return ids, y\n",
        "    \n",
        "    def preprocess(self, text):\n",
        "        tokens = text.lower().split()\n",
        "        tokens = [token.strip(punctuation) for token in tokens]\n",
        "        tokens = [token for token in tokens if token not in punctuation]\n",
        "        return tokens\n",
        "\n",
        "    def collate_fn(self, batch): #этот метод можно реализовывать и отдельно,\n",
        "    # он понадобится для DataLoader во время итерации по батчам\n",
        "      ids, y = list(zip(*batch))\n",
        "      padded_ids = pad_sequence(ids, batch_first=True).to(self.device)\n",
        "      y = torch.Tensor(y).to(self.device)\n",
        "      return padded_ids,  y"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWlFR-TCuxRR"
      },
      "source": [
        "train_dataset = TweetsDataset(train_sentences, word2id, DEVICE)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_iterator = DataLoader(train_dataset, collate_fn = train_dataset.collate_fn, sampler=train_sampler, batch_size=64)\n",
        "val_dataset = TweetsDataset(val_sentences, word2id, DEVICE)\n",
        "val_sampler = SequentialSampler(val_dataset)\n",
        "val_iterator = DataLoader(val_dataset, collate_fn = val_dataset.collate_fn, sampler=val_sampler, batch_size=64)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVhLPBPxS2gI"
      },
      "source": [
        "Теперь зададим архитектуру модели:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mgdh6gVVyMYj"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim, pretrained=False):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        if pretrained:\n",
        "          self.embedding.from_pretrained(torch.tensor(weights), freeze=True)\n",
        "        self.bigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=100, kernel_size=2, padding='same')\n",
        "        self.trigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=80, kernel_size=3, padding='same')\n",
        "        self.conv = nn.Conv1d(in_channels=180, out_channels=100, kernel_size=2)\n",
        "        self.pooling = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.hidden = nn.Linear(in_features=100, out_features=1)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.out = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, word):\n",
        "        #batch_size x seq_len\n",
        "        embedded = self.embedding(word)\n",
        "        embedded = embedded.transpose(1,2)\n",
        "        bigrams = self.relu(self.bigrams(embedded))\n",
        "        trigrams = self.relu(self.trigrams(embedded))\n",
        "        concat = torch.cat((bigrams, trigrams), 1)\n",
        "        conv = self.pooling(self.conv(concat)).max(2)[0]\n",
        "        logits = self.hidden(conv) \n",
        "        logits = self.out(logits)      \n",
        "        return logits"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFPNnwskFVIQ"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, metric):\n",
        "    print('Training...')\n",
        "    epoch_loss = 0 # для подсчета среднего лосса на всех батчах\n",
        "    epoch_metric = 0\n",
        "    model.train()  # ставим модель в обучение, явно указываем, что сейчас надо будет хранить градиенты у всех весов\n",
        "\n",
        "    for i, (texts, ys) in enumerate(iterator): #итерируемся по батчам\n",
        "        optimizer.zero_grad()  #обнуляем градиенты\n",
        "        preds = model(texts).squeeze()  #прогоняем данные через модель\n",
        "        loss = criterion(preds, ys) #считаем значение функции потерь  \n",
        "        loss.backward() #считаем градиенты  \n",
        "        optimizer.step() #обновляем веса \n",
        "        epoch_loss += loss.item() #сохраняем значение функции потерь\n",
        "        \n",
        "        batch_metric = metric(preds, ys.long())\n",
        "        epoch_metric += batch_metric\n",
        "        #if not (i + 1) % 20:\n",
        "        #    print(f'Train loss: {loss.item()}, Train f1: {batch_metric}')\n",
        "            #print(f'Train loss: {epoch_loss/i}, Train f1: {epoch_metric/i}')     \n",
        "    print(f'F-score: {epoch_metric / len(iterator)}, loss: {epoch_loss / len(iterator)}') \n",
        "    return epoch_metric / len(iterator), epoch_loss / len(iterator) # возвращаем среднее значение по всей выборке"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaytxpSoCHoj"
      },
      "source": [
        "def evaluate(model, iterator, criterion, metric):\n",
        "    print(\"\\nValidating...\")\n",
        "    epoch_loss = 0\n",
        "    epoch_metric = 0\n",
        "    model.eval() \n",
        "    with torch.no_grad():\n",
        "        for i, (texts, ys) in enumerate(iterator):   \n",
        "            preds = model(texts).squeeze()  # делаем предсказания на тесте\n",
        "            loss = criterion(preds, ys)   # считаем значения функции ошибки для статистики  \n",
        "            epoch_loss += loss.item()\n",
        "            batch_metric = metric(preds, ys.long())\n",
        "            epoch_metric += batch_metric\n",
        "\n",
        "    print(f'F-score: {epoch_metric / len(iterator)}, loss: {epoch_loss / len(iterator)}')\n",
        "        \n",
        "    return epoch_metric / len(iterator), epoch_loss / len(iterator) # возвращаем среднее значение по всей выборке"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKwckOfDJ1k6"
      },
      "source": [
        "def train_model(model, lr, epochs):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.BCELoss() \n",
        "\n",
        "    model = model.to(DEVICE)\n",
        "    criterion = criterion.to(DEVICE)\n",
        "\n",
        "    losses = []\n",
        "    losses_eval = []\n",
        "    f1s = []\n",
        "    f1s_eval = []\n",
        "\n",
        "    for i in range(epochs):\n",
        "        print(f'\\nstarting Epoch {i}')\n",
        "        f1_value, epoch_loss = train(model, train_iterator, optimizer, criterion, metric=f1)\n",
        "        losses.append(epoch_loss)\n",
        "        f1s.append(f1_value)\n",
        "\n",
        "        f1_value_on_test, epoch_loss_on_test = evaluate(model, val_iterator, criterion, metric=f1)\n",
        "        losses_eval.append(epoch_loss_on_test)\n",
        "        f1s_eval.append(f1_value_on_test)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZBImxVpS76b"
      },
      "source": [
        "Тренируем модель с дефолтными параметрами:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfWS3CwNCMnI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e1d9f6b-e504-4757-c7b6-0aa101b4ce68"
      },
      "source": [
        "model = CNN(len(word2id), 8)\n",
        "train_model(model, 0.001, 3)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py:298: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at  ../aten/src/ATen/native/Convolution.cpp:647.)\n",
            "  self.padding, self.dilation, self.groups)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-score: 0.6242083311080933, loss: 0.6217950288369222\n",
            "\n",
            "Validating...\n",
            "F-score: 0.6830612421035767, loss: 0.5853740812945265\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "F-score: 0.7102341651916504, loss: 0.545849538530382\n",
            "\n",
            "Validating...\n",
            "F-score: 0.7305583953857422, loss: 0.5490723389328283\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "F-score: 0.7540391087532043, loss: 0.4919508190980583\n",
            "\n",
            "Validating...\n",
            "F-score: 0.7225967645645142, loss: 0.5226638578561532\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlIWZzE2UTbt"
      },
      "source": [
        "F-score равен 0.72, поэтому есть куда улучшать!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ix_riiS_I3Fa"
      },
      "source": [
        "### 1.2. Используем  эмбеддинги FastText"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-dsodLxUiRF"
      },
      "source": [
        "Попробуем натренировать FastText на нашем корпусе и загрузить эмбеддинги в модель"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXy5h2juI-2m"
      },
      "source": [
        "from gensim.models import FastText"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcaLBCQuJixN"
      },
      "source": [
        "texts = all_tweets_data.text.apply(preprocess).to_list()"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54zagE2DJWkX"
      },
      "source": [
        "model = FastText(texts, size=100, window=5, min_count=1)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ru0IKqYhLBt3"
      },
      "source": [
        "weights = np.zeros((len(word2id), 100))\n",
        "count = 0\n",
        "for word, i in word2id.items():\n",
        "    if word == 'PAD':\n",
        "        continue   \n",
        "    try:\n",
        "        weights[i] = model.wv[word]    \n",
        "    except KeyError:\n",
        "      count += 1\n",
        "      # oov словам сопоставляем случайный вектор\n",
        "      weights[i] = np.random.normal(0,0.1,100)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcUtRivBLgm0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d98b921-9f8e-4450-df38-b9777e47a123"
      },
      "source": [
        "fasttext_model = CNN(len(word2id), 100, pretrained=True)\n",
        "train_model(fasttext_model, 0.001, 3)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "F-score: 0.6879616975784302, loss: 0.5653806128480034\n",
            "\n",
            "Validating...\n",
            "F-score: 0.74798583984375, loss: 0.5266508212462803\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "F-score: 0.7758302688598633, loss: 0.45974038883896573\n",
            "\n",
            "Validating...\n",
            "F-score: 0.7539206743240356, loss: 0.4945674329281526\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "F-score: 0.824968159198761, loss: 0.38293773979368934\n",
            "\n",
            "Validating...\n",
            "F-score: 0.7707210779190063, loss: 0.5062397499851179\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lo2jQX6Vr95z"
      },
      "source": [
        "FastText помог улучшить качество! Вероятно, модели не хватило эпох, чтобы обучить эмбеддинги, но она лучше работает с предобученными"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6mi7i2Vpn30"
      },
      "source": [
        "### Меняем гиперпараметры"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVxSaBSZk-hj"
      },
      "source": [
        "Увеличим размер эмбеддинга:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qyz18xSypmYs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abea7c32-b93e-4efa-ed8f-6db1c60767fa"
      },
      "source": [
        "model = CNN(len(word2id), 50)\n",
        "train_model(model, 0.001, 3)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "F-score: 0.6716139912605286, loss: 0.5812360862603645\n",
            "\n",
            "Validating...\n",
            "F-score: 0.7122611403465271, loss: 0.5352640655548515\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "F-score: 0.7596361637115479, loss: 0.48372277561759075\n",
            "\n",
            "Validating...\n",
            "F-score: 0.7534310817718506, loss: 0.5062956153001368\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "F-score: 0.8064237833023071, loss: 0.41261900751227554\n",
            "\n",
            "Validating...\n",
            "F-score: 0.7586255073547363, loss: 0.5044789119651186\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNFPmhwBsXJg"
      },
      "source": [
        "В целом увеличение эмбеддинга дало лучший результат, так как можно хранить больше информации. Это улучшает качество классификации!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uj5Ovm4-lKli"
      },
      "source": [
        "Теперь уменьшим лернинг рейт:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpNKkSBYIoL_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80dea43a-f861-4b76-db2c-b0edc7d1c826"
      },
      "source": [
        "model = CNN(len(word2id), 8)\n",
        "train_model(model, 0.0005, 3)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "F-score: 0.6075225472450256, loss: 0.643439017751153\n",
            "\n",
            "Validating...\n",
            "F-score: 0.6385930776596069, loss: 0.6080097086170665\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "F-score: 0.67527836561203, loss: 0.583494718482111\n",
            "\n",
            "Validating...\n",
            "F-score: 0.690639853477478, loss: 0.5764812672003704\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "F-score: 0.7140663862228394, loss: 0.5424197400553606\n",
            "\n",
            "Validating...\n",
            "F-score: 0.6893985867500305, loss: 0.557993971284253\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZfKh-Oss0ow"
      },
      "source": [
        "Уменьшение лернинг рейта испортила качество модели, так как, вероятно, модель не дошла до локального минимума, двигаясь со слишком маленьким шагом"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwNbbGYlMYff"
      },
      "source": [
        "### 3. Изменим предобработку"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqollsjtlVn4"
      },
      "source": [
        "Теперь попробуем прогнать нашу модель на лемматизированных текстах:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Te3NcV7RB0kU"
      },
      "source": [
        "train_dataset = TweetsDataset(train_sentences, lemma2id, DEVICE, text='lemmas')\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_iterator = DataLoader(train_dataset, collate_fn = train_dataset.collate_fn, sampler=train_sampler, batch_size=64)\n",
        "val_dataset = TweetsDataset(val_sentences, lemma2id, DEVICE, text='lemmas')\n",
        "val_sampler = SequentialSampler(val_dataset)\n",
        "val_iterator = DataLoader(val_dataset, collate_fn = val_dataset.collate_fn, sampler=val_sampler, batch_size=64)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_dPxqTSCCBt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd04975f-0389-4e10-a51c-a2d48bd6191e"
      },
      "source": [
        "model = CNN(len(lemma2id), 10)\n",
        "train_model(model, 0.001, 3)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "F-score: 0.6683898568153381, loss: 0.5769473152348118\n",
            "\n",
            "Validating...\n",
            "F-score: 0.7003044486045837, loss: 0.5321198603077903\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "F-score: 0.7483500242233276, loss: 0.49594231518392673\n",
            "\n",
            "Validating...\n",
            "F-score: 0.7401742935180664, loss: 0.49363468753434037\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "F-score: 0.7804466485977173, loss: 0.4506233713900925\n",
            "\n",
            "Validating...\n",
            "F-score: 0.7715306282043457, loss: 0.4832143023589098\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGvAVrSttRcy"
      },
      "source": [
        "С леммами повышается качество классификации, что ожидаемо, так как теперь у него больше данных про конкретное слово, потому что токены объединились в меньшее количество лемм"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCZAa9EY4eOg"
      },
      "source": [
        "# Часть II.\n",
        "\n",
        "Зададим датасет с двумя выходами:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjUkHKN8yz_h"
      },
      "source": [
        "class TwoWayTweetsDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataset, word2id, symbol2id, DEVICE):\n",
        "        self.dataset = dataset['text'].values\n",
        "        self.word2id = word2id\n",
        "        self.symbol2id = symbol2id\n",
        "        self.length = dataset.shape[0]\n",
        "        self.target = torch.Tensor(dataset['tone'].values)\n",
        "        self.device = DEVICE\n",
        "\n",
        "    def __len__(self): #это обязательный метод, он должен уметь считать длину датасета\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, index): #еще один обязательный метод. По индексу возвращает элемент выборки\n",
        "        tokens = self.tokenize(self.dataset[index]) # токенизируем\n",
        "        symbols = list(self.dataset[index])\n",
        "        ids = torch.LongTensor([self.word2id[token] for token in tokens if token in self.word2id])\n",
        "        symbol_ids = torch.LongTensor([self.symbol2id[symbol] for symbol in symbols if symbol in self.symbol2id])\n",
        "        y = self.target[index]\n",
        "        return ids, symbol_ids, y\n",
        "    \n",
        "    def preprocess(self, text):\n",
        "        tokens = text.lower().split()\n",
        "        tokens = [token.strip(punctuation) for token in tokens]\n",
        "        tokens = [token for token in tokens if token not in punctuation]\n",
        "        return tokens\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        tokens = [token for token in tokenizer.tokenize(text.lower()) if token not in punctuation]\n",
        "        #lemmas = [morph.parse(token)[0].normal_form for token in tokens if token not in punctuation]\n",
        "        return tokens\n",
        "\n",
        "    def collate_fn(self, batch): #этот метод можно реализовывать и отдельно,\n",
        "    # он понадобится для DataLoader во время итерации по батчам\n",
        "      ids, symbol_ids, y = list(zip(*batch))\n",
        "      padded_ids = pad_sequence(ids, batch_first=True).to(self.device)\n",
        "      padded_symbol_ids = pad_sequence(symbol_ids, batch_first=True).to(self.device)\n",
        "      y = torch.Tensor(y).to(self.device)\n",
        "      return padded_ids, padded_symbol_ids, y"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utpUTBte3-Gj"
      },
      "source": [
        "class MixedModel(nn.Module):\n",
        "    \n",
        "    def __init__(self, word_vocab_size, symbol_vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.word_embedding = nn.Embedding(word_vocab_size, embedding_dim)\n",
        "        self.symbol_embedding = nn.Embedding(symbol_vocab_size, embedding_dim)\n",
        "        self.linear = nn.Linear(in_features=embedding_dim, out_features=10)\n",
        "        self.bigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=10, kernel_size=2, padding='same')\n",
        "        self.trigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=8, kernel_size=3, padding='same')\n",
        "        self.pooling = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.hidden = nn.Linear(in_features=28, out_features=1)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.out = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, word, symbol):\n",
        "        word_embedded = self.word_embedding(word)\n",
        "        sentence = torch.mean(word_embedded, axis=1)\n",
        "        linear = self.linear(sentence)\n",
        "        symbol_embedded = self.symbol_embedding(symbol)\n",
        "        embedded = symbol_embedded.transpose(1,2)\n",
        "        feature_map_bigrams = self.pooling(self.relu(self.bigrams(embedded)))\n",
        "        feature_map_trigrams = self.pooling(self.relu(self.trigrams(embedded)))\n",
        "        pooling1 = feature_map_bigrams.max(2)[0] \n",
        "        pooling2 = feature_map_trigrams.max(2)[0]\n",
        "        concat = torch.cat((pooling1, pooling2, linear), 1)\n",
        "        logits = self.hidden(concat)\n",
        "        logits = self.out(logits)      \n",
        "        return logits"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wymRbDltG9TS"
      },
      "source": [
        "train_dataset = TwoWayTweetsDataset(train_sentences, lemma2id, symbol2id, DEVICE)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_iterator = DataLoader(train_dataset, collate_fn = train_dataset.collate_fn, sampler=train_sampler, batch_size=512)\n",
        "val_dataset = TwoWayTweetsDataset(val_sentences, lemma2id, symbol2id, DEVICE)\n",
        "val_sampler = SequentialSampler(val_dataset)\n",
        "val_iterator = DataLoader(val_dataset, collate_fn = val_dataset.collate_fn, sampler=val_sampler, batch_size=512)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDENIZ0LB7cu"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, metric):\n",
        "    print('Training...')\n",
        "    epoch_loss = 0 # для подсчета среднего лосса на всех батчах\n",
        "    epoch_metric = 0\n",
        "    model.train()\n",
        "\n",
        "    for i, (texts, symbols, ys) in enumerate(iterator): #итерируемся по батчам\n",
        "        optimizer.zero_grad()  #обнуляем градиенты\n",
        "        preds = model(texts, symbols).squeeze()  #прогоняем данные через модель\n",
        "        loss = criterion(preds, ys) #считаем значение функции потерь  \n",
        "        loss.backward() #считаем градиенты  \n",
        "        optimizer.step() #обновляем веса \n",
        "        epoch_loss += loss.item() #сохраняем значение функции потерь\n",
        "        batch_metric = metric(preds.round().long(), ys.long())\n",
        "        epoch_metric += batch_metric\n",
        "        \n",
        "    print(f'F-score: {epoch_metric / len(iterator)}, loss: {epoch_loss / len(iterator)}')\n",
        "        \n",
        "    return epoch_metric / len(iterator), epoch_loss / len(iterator)# возвращаем среднее значение по всей выборке"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jT-HjzQjFSRW"
      },
      "source": [
        "def evaluate(model, iterator, criterion, metric):\n",
        "    print(\"\\nValidating...\")\n",
        "    epoch_loss = 0\n",
        "    epoch_metric = 0\n",
        "    model.eval() \n",
        "    with torch.no_grad():\n",
        "        for i, (texts, symbols, ys) in enumerate(iterator):   \n",
        "            preds = model(texts, symbols).squeeze()  # делаем предсказания на тесте\n",
        "            loss = criterion(preds, ys)   # считаем значения функции ошибки для статистики  \n",
        "            epoch_loss += loss.item()\n",
        "            batch_metric = metric(preds.round().long(), ys.long())\n",
        "            epoch_metric += batch_metric\n",
        "\n",
        "    print(f'F-score: {epoch_metric / len(iterator)}, loss: {epoch_loss / len(iterator)}')\n",
        "        \n",
        "    return epoch_metric / len(iterator), epoch_loss / len(iterator)# возвращаем среднее значение по всей выборке"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zvezw6i2t_ca"
      },
      "source": [
        "В целом качество этой модели лучше, чем предыдущей, даже с меньшим лернинг рейтом"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EE7tyL0_0bJb",
        "outputId": "fe7dad95-60bb-489d-ce68-5b212a33f7f6"
      },
      "source": [
        "model = MixedModel(len(lemma2id), len(symbol2id), 8)\n",
        "train_model(model, 0.0001, 3)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "F-score: 0.5283611416816711, loss: 0.6911715675407732\n",
            "\n",
            "Validating...\n",
            "F-score: 0.5676530003547668, loss: 0.6839378509628639\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "F-score: 0.6234907507896423, loss: 0.6717464334528211\n",
            "\n",
            "Validating...\n",
            "F-score: 0.7041453123092651, loss: 0.6528091651670048\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "F-score: 0.7885995507240295, loss: 0.6144483237199381\n",
            "\n",
            "Validating...\n",
            "F-score: 0.8563372492790222, loss: 0.5650047077221817\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yx8fig1h0eaI",
        "outputId": "9fc2580b-29ce-435f-ea8e-87efde0401b7"
      },
      "source": [
        "model = MixedModel(len(lemma2id), len(symbol2id), 8)\n",
        "train_model(model, 0.001, 3)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "F-score: 0.863972544670105, loss: 0.37670726998591086\n",
            "\n",
            "Validating...\n",
            "F-score: 0.9870110750198364, loss: 0.07366102942255105\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "F-score: 0.9970766305923462, loss: 0.025617957445727268\n",
            "\n",
            "Validating...\n",
            "F-score: 0.9994733333587646, loss: 0.008515787420773439\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "F-score: 0.9994387626647949, loss: 0.005774067828274319\n",
            "\n",
            "Validating...\n",
            "F-score: 0.9996927976608276, loss: 0.0035480288186979093\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKTrxn_90niD"
      },
      "source": [
        "train_dataset = TwoWayTweetsDataset(train_sentences, word2id, symbol2id, DEVICE)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_iterator = DataLoader(train_dataset, collate_fn = train_dataset.collate_fn, sampler=train_sampler, batch_size=512)\n",
        "val_dataset = TwoWayTweetsDataset(val_sentences, word2id, symbol2id, DEVICE)\n",
        "val_sampler = SequentialSampler(val_dataset)\n",
        "val_iterator = DataLoader(val_dataset, collate_fn = val_dataset.collate_fn, sampler=val_sampler, batch_size=512)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZo_8YbP0kH3",
        "outputId": "8044b6f6-e682-47c2-99c1-b240b4d695da"
      },
      "source": [
        "model = MixedModel(len(word2id), len(symbol2id), 8)\n",
        "train_model(model, 0.001, 3)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "F-score: 0.905828058719635, loss: 0.31059286847290857\n",
            "\n",
            "Validating...\n",
            "F-score: 0.9785996675491333, loss: 0.062060048615329724\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "F-score: 0.9871753454208374, loss: 0.037329780223818734\n",
            "\n",
            "Validating...\n",
            "F-score: 0.9963210225105286, loss: 0.017604546297048586\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "F-score: 0.998002827167511, loss: 0.011016009926376209\n",
            "\n",
            "Validating...\n",
            "F-score: 0.9989983439445496, loss: 0.006635223790019584\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtkLeB3NubWe"
      },
      "source": [
        "При увеличении лернинг рейта увеличивается качество! При этом качество хорошее и на леммах, и на токенах"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-aX-vuuuQ-j"
      },
      "source": [
        "## Анализ\n",
        "\n",
        "1. Модель с символьными и пословными эмбеддингами работает лучше: вероятно, это можно объяснить, что не вся пунктуация убирается, и некоторые символьные значения помогают однозначно определять тональность\n",
        "\n",
        "2. Для обеих моделей оптимален learning rate 0.001, при таком лернинг рейте они достигают минимума (или приближаются к нему), при меньшем lr шаг слишком маленький.\n",
        "\n",
        "3. Предобученные эмбеддинги работают лучше, чем обучаемые внутри модели. Вероятно, модель не успевает за три эпохи хорошо обучить эмбеддинги, поэтому ей помогают предобученные эмбеддинги.\n",
        "\n",
        "4. Увеличение размера эмбеддингов увеличивает качество, так как хранится больше информации.\n",
        "\n",
        "5. С леммами модель работает лучше, так как объединяет токены в одну лемму и получает о ней больше информации"
      ]
    }
  ]
}